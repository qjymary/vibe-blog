"""
æ™ºèƒ½çŸ¥è¯†æºæœç´¢æœåŠ¡ - æ ¹æ®ä¸»é¢˜æ™ºèƒ½è·¯ç”±åˆ°ä¸åŒæœç´¢æº
"""

import json
import logging
import os
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from .search_service import get_search_service
from .arxiv_service import get_arxiv_service

logger = logging.getLogger(__name__)

# ä¸“ä¸šåšå®¢ç½‘ç«™é…ç½®
PROFESSIONAL_BLOGS = {
    'langchain': {
        'site': 'blog.langchain.dev',
        'name': 'LangChain Blog',
        'keywords': ['langchain', 'langgraph', 'lcel', 'langsmith']
    },
    'anthropic': {
        'site': 'anthropic.com',
        'name': 'Anthropic Research',
        'keywords': ['claude', 'anthropic', 'constitutional ai', 'rlhf']
    },
    'openai': {
        'site': 'openai.com',
        'name': 'OpenAI Blog',
        'keywords': ['gpt', 'chatgpt', 'openai', 'dall-e', 'whisper']
    },
    'jiqizhixin': {
        'site': 'jiqizhixin.com',
        'name': 'æœºå™¨ä¹‹å¿ƒ',
        'keywords': ['æœºå™¨ä¹‹å¿ƒ', 'ä¸­æ–‡', 'aièµ„è®¯']
    }
}

# å…¨å±€æœåŠ¡å®ä¾‹
_smart_search_service: Optional['SmartSearchService'] = None


class SmartSearchService:
    """
    æ™ºèƒ½æœç´¢æœåŠ¡ - æ ¹æ®ä¸»é¢˜æ™ºèƒ½é€‰æ‹©æœç´¢æº
    """
    
    def __init__(self, llm_client=None):
        """
        åˆå§‹åŒ–æ™ºèƒ½æœç´¢æœåŠ¡
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯ï¼Œç”¨äºæ™ºèƒ½è·¯ç”±
        """
        self.llm = llm_client
        self.max_workers = int(os.environ.get('BLOG_GENERATOR_MAX_WORKERS', '3'))
    
    def search(self, topic: str, article_type: str = '', max_results_per_source: int = 5) -> Dict[str, Any]:
        """
        æ™ºèƒ½æœç´¢ - æ ¹æ®ä¸»é¢˜é€‰æ‹©æœç´¢æºå¹¶å¹¶è¡Œæ‰§è¡Œ
        
        Args:
            topic: æœç´¢ä¸»é¢˜
            article_type: æ–‡ç« ç±»å‹
            max_results_per_source: æ¯ä¸ªæºçš„æœ€å¤§ç»“æœæ•°
            
        Returns:
            åˆå¹¶åçš„æœç´¢ç»“æœ
        """
        logger.info(f"ğŸ§  æ™ºèƒ½æœç´¢å¼€å§‹: {topic}")
        
        # ç¬¬ä¸€æ­¥ï¼šLLM åˆ¤æ–­éœ€è¦å“ªäº›æœç´¢æº
        routing_result = self._route_search_sources(topic)
        
        sources = routing_result.get('sources', ['general'])
        arxiv_query = routing_result.get('arxiv_query', topic)
        blog_query = routing_result.get('blog_query', topic)
        
        logger.info(f"ğŸ§  æœç´¢æºè·¯ç”±ç»“æœ: {sources}")
        
        # ç¬¬äºŒæ­¥ï¼šå¹¶è¡Œæ‰§è¡Œæœç´¢
        all_results = []
        search_tasks = []
        
        # å‡†å¤‡æœç´¢ä»»åŠ¡
        if 'arxiv' in sources:
            search_tasks.append(('arxiv', arxiv_query))
        
        # ä¸“ä¸šåšå®¢æœç´¢
        for source in sources:
            if source in PROFESSIONAL_BLOGS:
                search_tasks.append(('blog', source, blog_query))
        
        # é€šç”¨æœç´¢ï¼ˆå§‹ç»ˆåŒ…å«ï¼‰
        if 'general' in sources or not search_tasks:
            search_tasks.append(('general', blog_query))
        
        # å¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {}
            
            for task in search_tasks:
                if task[0] == 'arxiv':
                    future = executor.submit(self._search_arxiv, task[1], max_results_per_source)
                    futures[future] = 'arxiv'
                elif task[0] == 'blog':
                    future = executor.submit(self._search_blog, task[1], task[2], max_results_per_source)
                    futures[future] = f'blog:{task[1]}'
                elif task[0] == 'general':
                    future = executor.submit(self._search_general, task[1], max_results_per_source)
                    futures[future] = 'general'
            
            for future in as_completed(futures):
                source_name = futures[future]
                try:
                    result = future.result()
                    if result.get('success') and result.get('results'):
                        all_results.extend(result['results'])
                        logger.info(f"âœ… {source_name} æœç´¢å®Œæˆ: {len(result['results'])} æ¡ç»“æœ")
                except Exception as e:
                    logger.error(f"âŒ {source_name} æœç´¢å¤±è´¥: {e}")
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶å»é‡
        merged_results = self._merge_and_dedupe(all_results)
        
        logger.info(f"ğŸ§  æ™ºèƒ½æœç´¢å®Œæˆ: å…± {len(merged_results)} æ¡ç»“æœ")
        
        return {
            'success': True,
            'results': merged_results,
            'summary': self._generate_summary(merged_results),
            'sources_used': sources,
            'error': None
        }
    
    def _route_search_sources(self, topic: str) -> Dict[str, Any]:
        """ä½¿ç”¨ LLM åˆ¤æ–­éœ€è¦å“ªäº›æœç´¢æº"""
        if not self.llm:
            # æ—  LLM æ—¶ä½¿ç”¨ç®€å•è§„åˆ™åŒ¹é…
            return self._rule_based_routing(topic)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœç´¢æºè·¯ç”±å™¨ã€‚æ ¹æ®ç”¨æˆ·çš„æŠ€æœ¯ä¸»é¢˜ï¼Œåˆ¤æ–­åº”è¯¥ä»å“ªäº›çŸ¥è¯†æºæœç´¢ã€‚

å¯ç”¨çš„æœç´¢æºï¼š
- arxiv: å­¦æœ¯è®ºæ–‡ï¼ˆæ¶‰åŠè®ºæ–‡ã€ç ”ç©¶ã€ç®—æ³•ã€æ¨¡å‹æ¶æ„ã€ç†è®ºæ—¶ä½¿ç”¨ï¼‰
- langchain: LangChain å®˜æ–¹åšå®¢ï¼ˆLangChainã€LangGraphã€LCELã€LangSmith ç›¸å…³ï¼‰
- anthropic: Anthropic ç ”ç©¶åšå®¢ï¼ˆClaudeã€Constitutional AIã€RLHF ç›¸å…³ï¼‰
- openai: OpenAI å®˜æ–¹åšå®¢ï¼ˆGPTã€ChatGPTã€DALL-E ç›¸å…³ï¼‰
- huggingface: Hugging Face åšå®¢ï¼ˆå¼€æºæ¨¡å‹ã€Transformersã€Diffusers ç›¸å…³ï¼‰
- jiqizhixin: æœºå™¨ä¹‹å¿ƒï¼ˆä¸­æ–‡ AI èµ„è®¯ã€è¡Œä¸šåŠ¨æ€ï¼‰
- general: é€šç”¨æœç´¢ï¼ˆå§‹ç»ˆåŒ…å«ï¼Œä½œä¸ºå…œåº•ï¼‰

ç”¨æˆ·ä¸»é¢˜: {topic}

è¯·è¿”å› JSON æ ¼å¼ï¼ˆåªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š
{{
  "sources": ["arxiv", "langchain", "general"],
  "arxiv_query": "è‹±æ–‡æœç´¢è¯ï¼Œç”¨äº arXiv è®ºæ–‡æœç´¢",
  "blog_query": "ä¸­æ–‡æˆ–è‹±æ–‡æœç´¢è¯ï¼Œç”¨äºåšå®¢æœç´¢"
}}"""

        try:
            response = self.llm.chat(
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response)
            
            # ç¡®ä¿ general å§‹ç»ˆåŒ…å«
            if 'general' not in result.get('sources', []):
                result['sources'].append('general')
            
            return result
            
        except Exception as e:
            logger.warning(f"LLM è·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŒ¹é…: {e}")
            return self._rule_based_routing(topic)
    
    def _rule_based_routing(self, topic: str) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„ç®€å•è·¯ç”±ï¼ˆLLM ä¸å¯ç”¨æ—¶çš„å¤‡é€‰ï¼‰"""
        topic_lower = topic.lower()
        sources = ['general']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ arXiv
        arxiv_keywords = ['è®ºæ–‡', 'paper', 'ç ”ç©¶', 'research', 'ç®—æ³•', 'algorithm', 'æ¨¡å‹', 'model', 'transformer', 'attention']
        if any(kw in topic_lower for kw in arxiv_keywords):
            sources.append('arxiv')
        
        # æ£€æŸ¥ä¸“ä¸šåšå®¢
        for blog_id, config in PROFESSIONAL_BLOGS.items():
            if any(kw in topic_lower for kw in config['keywords']):
                sources.append(blog_id)
        
        return {
            'sources': sources,
            'arxiv_query': topic,
            'blog_query': topic
        }
    
    def _search_arxiv(self, query: str, max_results: int) -> Dict[str, Any]:
        """æœç´¢ arXiv"""
        arxiv_service = get_arxiv_service()
        if arxiv_service:
            return arxiv_service.search(query, max_results)
        return {'success': False, 'results': [], 'error': 'arXiv æœåŠ¡ä¸å¯ç”¨'}
    
    def _search_blog(self, blog_id: str, query: str, max_results: int) -> Dict[str, Any]:
        """æœç´¢ä¸“ä¸šåšå®¢ï¼ˆä½¿ç”¨ site: é™å®šï¼‰"""
        search_service = get_search_service()
        if not search_service or not search_service.is_available():
            return {'success': False, 'results': [], 'error': 'æœç´¢æœåŠ¡ä¸å¯ç”¨'}
        
        blog_config = PROFESSIONAL_BLOGS.get(blog_id)
        if not blog_config:
            return {'success': False, 'results': [], 'error': f'æœªçŸ¥åšå®¢: {blog_id}'}
        
        # ä½¿ç”¨ site: é™å®šæœç´¢
        site_query = f"{query} site:{blog_config['site']}"
        logger.info(f"ğŸ“ ä¸“ä¸šåšå®¢æœç´¢: {site_query}")
        
        result = search_service.search(site_query, max_results)
        
        # æ ‡è®°æ¥æº
        if result.get('results'):
            for item in result['results']:
                item['source'] = blog_config['name']
        
        return result
    
    def _search_general(self, query: str, max_results: int) -> Dict[str, Any]:
        """é€šç”¨æœç´¢"""
        search_service = get_search_service()
        if search_service and search_service.is_available():
            result = search_service.search(query, max_results)
            # æ ‡è®°æ¥æº
            if result.get('results'):
                for item in result['results']:
                    if not item.get('source'):
                        item['source'] = 'é€šç”¨æœç´¢'
            return result
        return {'success': False, 'results': [], 'error': 'æœç´¢æœåŠ¡ä¸å¯ç”¨'}
    
    def _merge_and_dedupe(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶å»é‡æœç´¢ç»“æœ"""
        seen_urls = set()
        merged = []
        
        for item in results:
            url = item.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                merged.append(item)
            elif not url:
                # æ—  URL çš„ç»“æœä¹Ÿä¿ç•™ï¼ˆå¦‚æŸäº›æ‘˜è¦ï¼‰
                merged.append(item)
        
        return merged
    
    def _generate_summary(self, results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæœç´¢ç»“æœæ‘˜è¦"""
        if not results:
            return ''
        
        summary_parts = []
        for i, item in enumerate(results, 1):
            source = item.get('source', 'æœªçŸ¥æ¥æº')
            title = item.get('title', '')
            content = item.get('content', '')[:800]
            
            summary_parts.append(f"[{source}] {title}\n{content}")
        
        return '\n\n---\n\n'.join(summary_parts)


def init_smart_search_service(llm_client=None) -> SmartSearchService:
    """åˆå§‹åŒ–æ™ºèƒ½æœç´¢æœåŠ¡"""
    global _smart_search_service
    _smart_search_service = SmartSearchService(llm_client)
    logger.info("æ™ºèƒ½çŸ¥è¯†æºæœç´¢æœåŠ¡å·²åˆå§‹åŒ–")
    return _smart_search_service


def get_smart_search_service() -> Optional[SmartSearchService]:
    """è·å–æ™ºèƒ½æœç´¢æœåŠ¡å®ä¾‹"""
    return _smart_search_service
